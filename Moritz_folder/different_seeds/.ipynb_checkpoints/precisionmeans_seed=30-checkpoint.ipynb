{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all relevant packages\n",
    "import torch, torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from backpack import backpack, extend\n",
    "from backpack.extensions import DiagHessian\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.nn import functional as F\n",
    "\n",
    "s=42\n",
    "np.random.seed(s)\n",
    "torch.manual_seed(s)\n",
    "torch.cuda.manual_seed(s)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data wrangling: load MNIST\n",
    "MNIST_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "MNIST_train = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=MNIST_transform)\n",
    "\n",
    "\n",
    "MNIST_test = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# have a look at the data to verify\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    #plt.imshow(npimg)\n",
    "    plt.show()\n",
    "\n",
    "images = MNIST_train.data[:10].view(10, 1, 28, 28)\n",
    "imshow(torchvision.utils.make_grid(images, nrow=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_loader = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_train,\n",
    "    batch_size=128,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "mnist_test_loader = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the network\n",
    "def NN(num_classes=10):\n",
    "    \n",
    "    features = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(1, 32, 5),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.MaxPool2d(2,2),\n",
    "        torch.nn.Conv2d(32, 32, 5),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.MaxPool2d(2,2),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(4 * 4 * 32, num_classes)\n",
    "    )\n",
    "    return(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST_weights_seed=42.pth\n"
     ]
    }
   ],
   "source": [
    "#set up the training routine\n",
    "mnist_model = NN(num_classes=10)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "mnist_train_optimizer = torch.optim.Adam(mnist_model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "#dont use SGD, it is way worse than Adam here\n",
    "MNIST_PATH = \"MNIST_weights_seed={}.pth\".format(s)\n",
    "#print(MNIST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get accuracy\n",
    "def get_accuracy(output, targets):\n",
    "    \"\"\"Helper function to print the accuracy\"\"\"\n",
    "    predictions = output.argmax(dim=1, keepdim=True).view_as(targets)\n",
    "    return predictions.eq(targets).float().mean().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the training routine and save the model at FMNIST_PATH\n",
    "\n",
    "def train(verbose=False, num_iter=5):\n",
    "    max_len = len(mnist_train_loader)\n",
    "    for iter in range(num_iter):\n",
    "        for batch_idx, (x, y) in enumerate(mnist_train_loader):\n",
    "            output = mnist_model(x)\n",
    "\n",
    "            accuracy = get_accuracy(output, y)\n",
    "\n",
    "            loss = loss_function(output, y)\n",
    "            loss.backward()\n",
    "            mnist_train_optimizer.step()\n",
    "            mnist_train_optimizer.zero_grad()\n",
    "\n",
    "            if verbose:\n",
    "                if batch_idx % 10 == 0:\n",
    "                    print(\n",
    "                        \"Iteration {}; {}/{} \\t\".format(iter, batch_idx, max_len) +\n",
    "                        \"Minibatch Loss %.3f  \" % (loss) +\n",
    "                        \"Accuracy %.0f\" % (accuracy * 100) + \"%\"\n",
    "                    )\n",
    "\n",
    "    print(\"saving model at: {}\".format(MNIST_PATH))\n",
    "    torch.save(mnist_model.state_dict(), MNIST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0; 0/469 \tMinibatch Loss 2.320  Accuracy 9%\n",
      "Iteration 0; 10/469 \tMinibatch Loss 2.026  Accuracy 59%\n",
      "Iteration 0; 20/469 \tMinibatch Loss 1.334  Accuracy 71%\n",
      "Iteration 0; 30/469 \tMinibatch Loss 0.775  Accuracy 77%\n",
      "Iteration 0; 40/469 \tMinibatch Loss 0.457  Accuracy 87%\n",
      "Iteration 0; 50/469 \tMinibatch Loss 0.565  Accuracy 84%\n",
      "Iteration 0; 60/469 \tMinibatch Loss 0.486  Accuracy 86%\n",
      "Iteration 0; 70/469 \tMinibatch Loss 0.337  Accuracy 89%\n",
      "Iteration 0; 80/469 \tMinibatch Loss 0.229  Accuracy 94%\n",
      "Iteration 0; 90/469 \tMinibatch Loss 0.467  Accuracy 87%\n",
      "Iteration 0; 100/469 \tMinibatch Loss 0.421  Accuracy 87%\n",
      "Iteration 0; 110/469 \tMinibatch Loss 0.277  Accuracy 94%\n",
      "Iteration 0; 120/469 \tMinibatch Loss 0.198  Accuracy 93%\n",
      "Iteration 0; 130/469 \tMinibatch Loss 0.334  Accuracy 91%\n",
      "Iteration 0; 140/469 \tMinibatch Loss 0.173  Accuracy 95%\n",
      "Iteration 0; 150/469 \tMinibatch Loss 0.073  Accuracy 99%\n",
      "Iteration 0; 160/469 \tMinibatch Loss 0.189  Accuracy 96%\n",
      "Iteration 0; 170/469 \tMinibatch Loss 0.087  Accuracy 99%\n",
      "Iteration 0; 180/469 \tMinibatch Loss 0.199  Accuracy 91%\n",
      "Iteration 0; 190/469 \tMinibatch Loss 0.176  Accuracy 93%\n",
      "Iteration 0; 200/469 \tMinibatch Loss 0.138  Accuracy 98%\n",
      "Iteration 0; 210/469 \tMinibatch Loss 0.124  Accuracy 97%\n",
      "Iteration 0; 220/469 \tMinibatch Loss 0.185  Accuracy 95%\n",
      "Iteration 0; 230/469 \tMinibatch Loss 0.108  Accuracy 97%\n",
      "Iteration 0; 240/469 \tMinibatch Loss 0.182  Accuracy 95%\n",
      "Iteration 0; 250/469 \tMinibatch Loss 0.129  Accuracy 97%\n",
      "Iteration 0; 260/469 \tMinibatch Loss 0.250  Accuracy 93%\n",
      "Iteration 0; 270/469 \tMinibatch Loss 0.133  Accuracy 95%\n",
      "Iteration 0; 280/469 \tMinibatch Loss 0.096  Accuracy 98%\n",
      "Iteration 0; 290/469 \tMinibatch Loss 0.067  Accuracy 97%\n",
      "Iteration 0; 300/469 \tMinibatch Loss 0.081  Accuracy 98%\n",
      "Iteration 0; 310/469 \tMinibatch Loss 0.116  Accuracy 98%\n",
      "Iteration 0; 320/469 \tMinibatch Loss 0.095  Accuracy 98%\n",
      "Iteration 0; 330/469 \tMinibatch Loss 0.094  Accuracy 98%\n",
      "Iteration 0; 340/469 \tMinibatch Loss 0.089  Accuracy 98%\n",
      "Iteration 0; 350/469 \tMinibatch Loss 0.197  Accuracy 95%\n",
      "Iteration 0; 360/469 \tMinibatch Loss 0.102  Accuracy 97%\n",
      "Iteration 0; 370/469 \tMinibatch Loss 0.148  Accuracy 95%\n",
      "Iteration 0; 380/469 \tMinibatch Loss 0.143  Accuracy 96%\n",
      "Iteration 0; 390/469 \tMinibatch Loss 0.090  Accuracy 98%\n",
      "Iteration 0; 400/469 \tMinibatch Loss 0.109  Accuracy 97%\n",
      "Iteration 0; 410/469 \tMinibatch Loss 0.187  Accuracy 95%\n",
      "Iteration 0; 420/469 \tMinibatch Loss 0.110  Accuracy 95%\n",
      "Iteration 0; 430/469 \tMinibatch Loss 0.141  Accuracy 93%\n",
      "Iteration 0; 440/469 \tMinibatch Loss 0.123  Accuracy 95%\n",
      "Iteration 0; 450/469 \tMinibatch Loss 0.062  Accuracy 98%\n",
      "Iteration 0; 460/469 \tMinibatch Loss 0.064  Accuracy 98%\n",
      "Iteration 1; 0/469 \tMinibatch Loss 0.063  Accuracy 99%\n",
      "Iteration 1; 10/469 \tMinibatch Loss 0.076  Accuracy 98%\n",
      "Iteration 1; 20/469 \tMinibatch Loss 0.114  Accuracy 97%\n",
      "Iteration 1; 30/469 \tMinibatch Loss 0.063  Accuracy 98%\n",
      "Iteration 1; 40/469 \tMinibatch Loss 0.072  Accuracy 99%\n",
      "Iteration 1; 50/469 \tMinibatch Loss 0.044  Accuracy 99%\n",
      "Iteration 1; 60/469 \tMinibatch Loss 0.020  Accuracy 100%\n",
      "Iteration 1; 70/469 \tMinibatch Loss 0.077  Accuracy 98%\n",
      "Iteration 1; 80/469 \tMinibatch Loss 0.045  Accuracy 98%\n",
      "Iteration 1; 90/469 \tMinibatch Loss 0.050  Accuracy 98%\n",
      "Iteration 1; 100/469 \tMinibatch Loss 0.037  Accuracy 99%\n",
      "Iteration 1; 110/469 \tMinibatch Loss 0.044  Accuracy 99%\n",
      "Iteration 1; 120/469 \tMinibatch Loss 0.045  Accuracy 99%\n",
      "Iteration 1; 130/469 \tMinibatch Loss 0.039  Accuracy 100%\n",
      "Iteration 1; 140/469 \tMinibatch Loss 0.058  Accuracy 99%\n",
      "Iteration 1; 150/469 \tMinibatch Loss 0.089  Accuracy 98%\n",
      "Iteration 1; 160/469 \tMinibatch Loss 0.038  Accuracy 100%\n",
      "Iteration 1; 170/469 \tMinibatch Loss 0.068  Accuracy 98%\n",
      "Iteration 1; 180/469 \tMinibatch Loss 0.037  Accuracy 98%\n",
      "Iteration 1; 190/469 \tMinibatch Loss 0.050  Accuracy 98%\n",
      "Iteration 1; 200/469 \tMinibatch Loss 0.111  Accuracy 95%\n",
      "Iteration 1; 210/469 \tMinibatch Loss 0.056  Accuracy 98%\n",
      "Iteration 1; 220/469 \tMinibatch Loss 0.061  Accuracy 98%\n",
      "Iteration 1; 230/469 \tMinibatch Loss 0.095  Accuracy 97%\n",
      "Iteration 1; 240/469 \tMinibatch Loss 0.036  Accuracy 100%\n",
      "Iteration 1; 250/469 \tMinibatch Loss 0.180  Accuracy 96%\n",
      "Iteration 1; 260/469 \tMinibatch Loss 0.057  Accuracy 98%\n",
      "Iteration 1; 270/469 \tMinibatch Loss 0.137  Accuracy 95%\n",
      "Iteration 1; 280/469 \tMinibatch Loss 0.080  Accuracy 98%\n",
      "Iteration 1; 290/469 \tMinibatch Loss 0.035  Accuracy 100%\n",
      "Iteration 1; 300/469 \tMinibatch Loss 0.064  Accuracy 98%\n",
      "Iteration 1; 310/469 \tMinibatch Loss 0.049  Accuracy 98%\n",
      "Iteration 1; 320/469 \tMinibatch Loss 0.059  Accuracy 98%\n",
      "Iteration 1; 330/469 \tMinibatch Loss 0.075  Accuracy 98%\n",
      "Iteration 1; 340/469 \tMinibatch Loss 0.042  Accuracy 99%\n",
      "Iteration 1; 350/469 \tMinibatch Loss 0.039  Accuracy 99%\n",
      "Iteration 1; 360/469 \tMinibatch Loss 0.098  Accuracy 96%\n",
      "Iteration 1; 370/469 \tMinibatch Loss 0.086  Accuracy 98%\n",
      "Iteration 1; 380/469 \tMinibatch Loss 0.138  Accuracy 95%\n",
      "Iteration 1; 390/469 \tMinibatch Loss 0.042  Accuracy 98%\n",
      "Iteration 1; 400/469 \tMinibatch Loss 0.047  Accuracy 98%\n",
      "Iteration 1; 410/469 \tMinibatch Loss 0.018  Accuracy 100%\n",
      "Iteration 1; 420/469 \tMinibatch Loss 0.075  Accuracy 98%\n",
      "Iteration 1; 430/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 1; 440/469 \tMinibatch Loss 0.071  Accuracy 98%\n",
      "Iteration 1; 450/469 \tMinibatch Loss 0.109  Accuracy 98%\n",
      "Iteration 1; 460/469 \tMinibatch Loss 0.058  Accuracy 98%\n",
      "Iteration 2; 0/469 \tMinibatch Loss 0.061  Accuracy 98%\n",
      "Iteration 2; 10/469 \tMinibatch Loss 0.067  Accuracy 98%\n",
      "Iteration 2; 20/469 \tMinibatch Loss 0.040  Accuracy 98%\n",
      "Iteration 2; 30/469 \tMinibatch Loss 0.045  Accuracy 98%\n",
      "Iteration 2; 40/469 \tMinibatch Loss 0.067  Accuracy 98%\n",
      "Iteration 2; 50/469 \tMinibatch Loss 0.050  Accuracy 98%\n",
      "Iteration 2; 60/469 \tMinibatch Loss 0.110  Accuracy 94%\n",
      "Iteration 2; 70/469 \tMinibatch Loss 0.035  Accuracy 99%\n",
      "Iteration 2; 80/469 \tMinibatch Loss 0.027  Accuracy 99%\n",
      "Iteration 2; 90/469 \tMinibatch Loss 0.150  Accuracy 96%\n",
      "Iteration 2; 100/469 \tMinibatch Loss 0.048  Accuracy 99%\n",
      "Iteration 2; 110/469 \tMinibatch Loss 0.058  Accuracy 98%\n",
      "Iteration 2; 120/469 \tMinibatch Loss 0.021  Accuracy 100%\n",
      "Iteration 2; 130/469 \tMinibatch Loss 0.031  Accuracy 98%\n",
      "Iteration 2; 140/469 \tMinibatch Loss 0.024  Accuracy 99%\n",
      "Iteration 2; 150/469 \tMinibatch Loss 0.023  Accuracy 100%\n",
      "Iteration 2; 160/469 \tMinibatch Loss 0.096  Accuracy 98%\n",
      "Iteration 2; 170/469 \tMinibatch Loss 0.059  Accuracy 98%\n",
      "Iteration 2; 180/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 2; 190/469 \tMinibatch Loss 0.099  Accuracy 98%\n",
      "Iteration 2; 200/469 \tMinibatch Loss 0.082  Accuracy 98%\n",
      "Iteration 2; 210/469 \tMinibatch Loss 0.055  Accuracy 98%\n",
      "Iteration 2; 220/469 \tMinibatch Loss 0.070  Accuracy 98%\n",
      "Iteration 2; 230/469 \tMinibatch Loss 0.089  Accuracy 98%\n",
      "Iteration 2; 240/469 \tMinibatch Loss 0.070  Accuracy 98%\n",
      "Iteration 2; 250/469 \tMinibatch Loss 0.036  Accuracy 98%\n",
      "Iteration 2; 260/469 \tMinibatch Loss 0.089  Accuracy 97%\n",
      "Iteration 2; 270/469 \tMinibatch Loss 0.057  Accuracy 98%\n",
      "Iteration 2; 280/469 \tMinibatch Loss 0.060  Accuracy 99%\n",
      "Iteration 2; 290/469 \tMinibatch Loss 0.080  Accuracy 98%\n",
      "Iteration 2; 300/469 \tMinibatch Loss 0.041  Accuracy 99%\n",
      "Iteration 2; 310/469 \tMinibatch Loss 0.030  Accuracy 99%\n",
      "Iteration 2; 320/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 2; 330/469 \tMinibatch Loss 0.014  Accuracy 99%\n",
      "Iteration 2; 340/469 \tMinibatch Loss 0.058  Accuracy 98%\n",
      "Iteration 2; 350/469 \tMinibatch Loss 0.050  Accuracy 97%\n",
      "Iteration 2; 360/469 \tMinibatch Loss 0.019  Accuracy 99%\n",
      "Iteration 2; 370/469 \tMinibatch Loss 0.030  Accuracy 99%\n",
      "Iteration 2; 380/469 \tMinibatch Loss 0.037  Accuracy 98%\n",
      "Iteration 2; 390/469 \tMinibatch Loss 0.056  Accuracy 98%\n",
      "Iteration 2; 400/469 \tMinibatch Loss 0.068  Accuracy 98%\n",
      "Iteration 2; 410/469 \tMinibatch Loss 0.058  Accuracy 98%\n",
      "Iteration 2; 420/469 \tMinibatch Loss 0.081  Accuracy 97%\n",
      "Iteration 2; 430/469 \tMinibatch Loss 0.079  Accuracy 98%\n",
      "Iteration 2; 440/469 \tMinibatch Loss 0.030  Accuracy 99%\n",
      "Iteration 2; 450/469 \tMinibatch Loss 0.061  Accuracy 98%\n",
      "Iteration 2; 460/469 \tMinibatch Loss 0.016  Accuracy 100%\n",
      "Iteration 3; 0/469 \tMinibatch Loss 0.062  Accuracy 98%\n",
      "Iteration 3; 10/469 \tMinibatch Loss 0.022  Accuracy 99%\n",
      "Iteration 3; 20/469 \tMinibatch Loss 0.076  Accuracy 97%\n",
      "Iteration 3; 30/469 \tMinibatch Loss 0.041  Accuracy 99%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3; 40/469 \tMinibatch Loss 0.033  Accuracy 98%\n",
      "Iteration 3; 50/469 \tMinibatch Loss 0.058  Accuracy 98%\n",
      "Iteration 3; 60/469 \tMinibatch Loss 0.031  Accuracy 99%\n",
      "Iteration 3; 70/469 \tMinibatch Loss 0.104  Accuracy 95%\n",
      "Iteration 3; 80/469 \tMinibatch Loss 0.057  Accuracy 98%\n",
      "Iteration 3; 90/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 3; 100/469 \tMinibatch Loss 0.037  Accuracy 98%\n",
      "Iteration 3; 110/469 \tMinibatch Loss 0.039  Accuracy 98%\n",
      "Iteration 3; 120/469 \tMinibatch Loss 0.031  Accuracy 99%\n",
      "Iteration 3; 130/469 \tMinibatch Loss 0.045  Accuracy 98%\n",
      "Iteration 3; 140/469 \tMinibatch Loss 0.081  Accuracy 97%\n",
      "Iteration 3; 150/469 \tMinibatch Loss 0.113  Accuracy 97%\n",
      "Iteration 3; 160/469 \tMinibatch Loss 0.035  Accuracy 98%\n",
      "Iteration 3; 170/469 \tMinibatch Loss 0.028  Accuracy 98%\n",
      "Iteration 3; 180/469 \tMinibatch Loss 0.059  Accuracy 98%\n",
      "Iteration 3; 190/469 \tMinibatch Loss 0.062  Accuracy 98%\n",
      "Iteration 3; 200/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 3; 210/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 3; 220/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 3; 230/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 3; 240/469 \tMinibatch Loss 0.026  Accuracy 100%\n",
      "Iteration 3; 250/469 \tMinibatch Loss 0.057  Accuracy 98%\n",
      "Iteration 3; 260/469 \tMinibatch Loss 0.034  Accuracy 99%\n",
      "Iteration 3; 270/469 \tMinibatch Loss 0.052  Accuracy 98%\n",
      "Iteration 3; 280/469 \tMinibatch Loss 0.039  Accuracy 99%\n",
      "Iteration 3; 290/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 3; 300/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 3; 310/469 \tMinibatch Loss 0.067  Accuracy 98%\n",
      "Iteration 3; 320/469 \tMinibatch Loss 0.116  Accuracy 96%\n",
      "Iteration 3; 330/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 3; 340/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 3; 350/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 3; 360/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 3; 370/469 \tMinibatch Loss 0.085  Accuracy 98%\n",
      "Iteration 3; 380/469 \tMinibatch Loss 0.022  Accuracy 100%\n",
      "Iteration 3; 390/469 \tMinibatch Loss 0.017  Accuracy 100%\n",
      "Iteration 3; 400/469 \tMinibatch Loss 0.033  Accuracy 98%\n",
      "Iteration 3; 410/469 \tMinibatch Loss 0.098  Accuracy 97%\n",
      "Iteration 3; 420/469 \tMinibatch Loss 0.025  Accuracy 100%\n",
      "Iteration 3; 430/469 \tMinibatch Loss 0.087  Accuracy 97%\n",
      "Iteration 3; 440/469 \tMinibatch Loss 0.035  Accuracy 98%\n",
      "Iteration 3; 450/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 3; 460/469 \tMinibatch Loss 0.068  Accuracy 97%\n",
      "Iteration 4; 0/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 4; 10/469 \tMinibatch Loss 0.032  Accuracy 98%\n",
      "Iteration 4; 20/469 \tMinibatch Loss 0.048  Accuracy 98%\n",
      "Iteration 4; 30/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 4; 40/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 4; 50/469 \tMinibatch Loss 0.072  Accuracy 98%\n",
      "Iteration 4; 60/469 \tMinibatch Loss 0.091  Accuracy 98%\n",
      "Iteration 4; 70/469 \tMinibatch Loss 0.053  Accuracy 98%\n",
      "Iteration 4; 80/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 4; 90/469 \tMinibatch Loss 0.050  Accuracy 98%\n",
      "Iteration 4; 100/469 \tMinibatch Loss 0.192  Accuracy 98%\n",
      "Iteration 4; 110/469 \tMinibatch Loss 0.025  Accuracy 100%\n",
      "Iteration 4; 120/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 4; 130/469 \tMinibatch Loss 0.026  Accuracy 98%\n",
      "Iteration 4; 140/469 \tMinibatch Loss 0.050  Accuracy 99%\n",
      "Iteration 4; 150/469 \tMinibatch Loss 0.033  Accuracy 98%\n",
      "Iteration 4; 160/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 4; 170/469 \tMinibatch Loss 0.047  Accuracy 98%\n",
      "Iteration 4; 180/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 4; 190/469 \tMinibatch Loss 0.078  Accuracy 98%\n",
      "Iteration 4; 200/469 \tMinibatch Loss 0.062  Accuracy 98%\n",
      "Iteration 4; 210/469 \tMinibatch Loss 0.056  Accuracy 98%\n",
      "Iteration 4; 220/469 \tMinibatch Loss 0.038  Accuracy 98%\n",
      "Iteration 4; 230/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 4; 240/469 \tMinibatch Loss 0.045  Accuracy 97%\n",
      "Iteration 4; 250/469 \tMinibatch Loss 0.019  Accuracy 99%\n",
      "Iteration 4; 260/469 \tMinibatch Loss 0.028  Accuracy 98%\n",
      "Iteration 4; 270/469 \tMinibatch Loss 0.073  Accuracy 98%\n",
      "Iteration 4; 280/469 \tMinibatch Loss 0.005  Accuracy 100%\n",
      "Iteration 4; 290/469 \tMinibatch Loss 0.037  Accuracy 98%\n",
      "Iteration 4; 300/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 4; 310/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 4; 320/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 4; 330/469 \tMinibatch Loss 0.112  Accuracy 98%\n",
      "Iteration 4; 340/469 \tMinibatch Loss 0.045  Accuracy 98%\n",
      "Iteration 4; 350/469 \tMinibatch Loss 0.024  Accuracy 99%\n",
      "Iteration 4; 360/469 \tMinibatch Loss 0.071  Accuracy 99%\n",
      "Iteration 4; 370/469 \tMinibatch Loss 0.023  Accuracy 100%\n",
      "Iteration 4; 380/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 4; 390/469 \tMinibatch Loss 0.033  Accuracy 99%\n",
      "Iteration 4; 400/469 \tMinibatch Loss 0.034  Accuracy 98%\n",
      "Iteration 4; 410/469 \tMinibatch Loss 0.048  Accuracy 98%\n",
      "Iteration 4; 420/469 \tMinibatch Loss 0.040  Accuracy 98%\n",
      "Iteration 4; 430/469 \tMinibatch Loss 0.052  Accuracy 98%\n",
      "Iteration 4; 440/469 \tMinibatch Loss 0.059  Accuracy 97%\n",
      "Iteration 4; 450/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 4; 460/469 \tMinibatch Loss 0.042  Accuracy 99%\n",
      "saving model at: MNIST_weights.pth\n"
     ]
    }
   ],
   "source": [
    "#after training it once, comment this out to save time if you rerun the entire script\n",
    "train(verbose=True, num_iter=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from: MNIST_weights.pth\n",
      "Batch 0/79 \tAccuracy 100%\n",
      "Batch 10/79 \tAccuracy 97%\n",
      "Batch 20/79 \tAccuracy 98%\n",
      "Batch 30/79 \tAccuracy 95%\n",
      "Batch 40/79 \tAccuracy 100%\n",
      "Batch 50/79 \tAccuracy 99%\n",
      "Batch 60/79 \tAccuracy 100%\n",
      "Batch 70/79 \tAccuracy 100%\n",
      "overall test accuracy on MNIST: 98.87 %\n"
     ]
    }
   ],
   "source": [
    "#predict in distribution\n",
    "MNIST_PATH = \"MNIST_weights.pth\"\n",
    "\n",
    "mnist_model = NN(num_classes=10)\n",
    "print(\"loading model from: {}\".format(MNIST_PATH))\n",
    "mnist_model.load_state_dict(torch.load(MNIST_PATH))\n",
    "mnist_model.eval()\n",
    "\n",
    "acc = []\n",
    "\n",
    "max_len = len(mnist_test_loader)\n",
    "for batch_idx, (x, y) in enumerate(mnist_test_loader):\n",
    "        output = mnist_model(x)\n",
    "        accuracy = get_accuracy(output, y)\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(\n",
    "                \"Batch {}/{} \\t\".format(batch_idx, max_len) + \n",
    "                \"Accuracy %.0f\" % (accuracy * 100) + \"%\"\n",
    "            )\n",
    "        acc.append(accuracy)\n",
    "    \n",
    "avg_acc = np.mean(acc)\n",
    "print('overall test accuracy on MNIST: {:.02f} %'.format(avg_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace approximation of the weights\n",
    "* we use the BackPACK package to approximate the Hessian of the parameters. Especially look at the DiagHessian() method.\n",
    "* we do one iteration over the entire training set and use the mean of the Hessian of the mini-batches as the best approximation of the Hessian.\n",
    "* we add a prior variance to our Hessian. The precision is 1 over the variance. we use a prior precision of 10, 20, and 50 (or variance of 1/10, 1/20, 1/50).\n",
    "    * edit: I will use precicisions of 10, 60, 120, 1000 in the following\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Hessian_NN(model, train_loader, prec0, device='cpu', verbose=True):\n",
    "    lossfunc = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    extend(lossfunc, debug=False)\n",
    "    extend(model, debug=False)\n",
    "\n",
    "    Hessian_diag = []\n",
    "    for name, param in model.named_parameters():\n",
    "        ps = param.size()\n",
    "        print(\"parameter size of \", name , ps)\n",
    "        Hessian_diag.append(torch.zeros(ps, device=device))\n",
    "        #print(param.numel())\n",
    "\n",
    "    var0 = 1/prec0\n",
    "    max_len = len(train_loader)\n",
    "\n",
    "    with backpack(DiagHessian()):\n",
    "\n",
    "        for batch_idx, (x, y) in enumerate(train_loader):\n",
    "\n",
    "            if device == 'cuda':\n",
    "                x, y = x.float().cuda(), y.long().cuda()\n",
    "\n",
    "            model.zero_grad()\n",
    "            lossfunc(model(x), y).backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Hessian of weight\n",
    "                for idx, param in enumerate(model.parameters()):\n",
    "\n",
    "                    H_ = param.diag_h\n",
    "                    #add prior here\n",
    "                    H_ += var0 * torch.ones(H_.size())\n",
    "\n",
    "                    rho = 1-1/(batch_idx+1)\n",
    "\n",
    "                    Hessian_diag[idx] = rho*Hessian_diag[idx] + (1-rho)* H_\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"Batch: {}/{}\".format(batch_idx, max_len))\n",
    "    \n",
    "    return(Hessian_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter size of  0.weight torch.Size([32, 1, 5, 5])\n",
      "parameter size of  0.bias torch.Size([32])\n",
      "parameter size of  3.weight torch.Size([32, 32, 5, 5])\n",
      "parameter size of  3.bias torch.Size([32])\n",
      "parameter size of  7.weight torch.Size([10, 512])\n",
      "parameter size of  7.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "#MNIST_NN_Hessian_diag_10 = get_Hessian_NN(model=mnist_model, train_loader=mnist_train_loader, prec0=10,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0.1007, 0.1007, 0.1007, 0.1006, 0.1004],\n",
      "          [0.1007, 0.1007, 0.1007, 0.1005, 0.1004],\n",
      "          [0.1007, 0.1007, 0.1007, 0.1004, 0.1004],\n",
      "          [0.1007, 0.1008, 0.1007, 0.1005, 0.1004],\n",
      "          [0.1008, 0.1008, 0.1007, 0.1005, 0.1004]]],\n",
      "\n",
      "\n",
      "        [[[0.1005, 0.1006, 0.1007, 0.1006, 0.1004],\n",
      "          [0.1005, 0.1005, 0.1006, 0.1005, 0.1004],\n",
      "          [0.1005, 0.1005, 0.1006, 0.1005, 0.1005],\n",
      "          [0.1004, 0.1004, 0.1005, 0.1005, 0.1006],\n",
      "          [0.1003, 0.1004, 0.1005, 0.1006, 0.1006]]],\n",
      "\n",
      "\n",
      "        [[[0.1015, 0.1021, 0.1029, 0.1037, 0.1033],\n",
      "          [0.1020, 0.1030, 0.1039, 0.1035, 0.1024],\n",
      "          [0.1026, 0.1037, 0.1038, 0.1023, 0.1013],\n",
      "          [0.1035, 0.1041, 0.1028, 0.1012, 0.1007],\n",
      "          [0.1041, 0.1040, 0.1024, 0.1010, 0.1007]]],\n",
      "\n",
      "\n",
      "        [[[0.1003, 0.1003, 0.1005, 0.1010, 0.1013],\n",
      "          [0.1002, 0.1004, 0.1009, 0.1015, 0.1015],\n",
      "          [0.1004, 0.1007, 0.1015, 0.1019, 0.1014],\n",
      "          [0.1007, 0.1014, 0.1022, 0.1020, 0.1011],\n",
      "          [0.1012, 0.1021, 0.1022, 0.1016, 0.1009]]],\n",
      "\n",
      "\n",
      "        [[[0.1027, 0.1031, 0.1023, 0.1010, 0.1005],\n",
      "          [0.1032, 0.1032, 0.1018, 0.1007, 0.1004],\n",
      "          [0.1042, 0.1039, 0.1021, 0.1011, 0.1010],\n",
      "          [0.1051, 0.1049, 0.1031, 0.1021, 0.1018],\n",
      "          [0.1055, 0.1053, 0.1040, 0.1028, 0.1024]]],\n",
      "\n",
      "\n",
      "        [[[0.1008, 0.1007, 0.1006, 0.1010, 0.1011],\n",
      "          [0.1007, 0.1005, 0.1006, 0.1011, 0.1013],\n",
      "          [0.1007, 0.1005, 0.1009, 0.1015, 0.1013],\n",
      "          [0.1010, 0.1011, 0.1016, 0.1017, 0.1011],\n",
      "          [0.1015, 0.1018, 0.1020, 0.1015, 0.1009]]],\n",
      "\n",
      "\n",
      "        [[[0.1005, 0.1006, 0.1007, 0.1006, 0.1007],\n",
      "          [0.1005, 0.1007, 0.1007, 0.1005, 0.1004],\n",
      "          [0.1006, 0.1007, 0.1006, 0.1004, 0.1003],\n",
      "          [0.1005, 0.1007, 0.1008, 0.1004, 0.1003],\n",
      "          [0.1005, 0.1007, 0.1009, 0.1006, 0.1004]]],\n",
      "\n",
      "\n",
      "        [[[0.1003, 0.1004, 0.1005, 0.1004, 0.1003],\n",
      "          [0.1004, 0.1004, 0.1004, 0.1003, 0.1003],\n",
      "          [0.1004, 0.1004, 0.1003, 0.1003, 0.1003],\n",
      "          [0.1004, 0.1003, 0.1004, 0.1004, 0.1004],\n",
      "          [0.1003, 0.1003, 0.1004, 0.1005, 0.1005]]],\n",
      "\n",
      "\n",
      "        [[[0.1033, 0.1031, 0.1036, 0.1046, 0.1052],\n",
      "          [0.1038, 0.1029, 0.1022, 0.1027, 0.1035],\n",
      "          [0.1064, 0.1041, 0.1021, 0.1022, 0.1024],\n",
      "          [0.1088, 0.1070, 0.1042, 0.1034, 0.1027],\n",
      "          [0.1088, 0.1093, 0.1067, 0.1048, 0.1035]]],\n",
      "\n",
      "\n",
      "        [[[0.1013, 0.1015, 0.1016, 0.1015, 0.1012],\n",
      "          [0.1014, 0.1016, 0.1014, 0.1012, 0.1010],\n",
      "          [0.1014, 0.1011, 0.1006, 0.1005, 0.1005],\n",
      "          [0.1011, 0.1007, 0.1004, 0.1003, 0.1005],\n",
      "          [0.1010, 0.1006, 0.1004, 0.1005, 0.1009]]],\n",
      "\n",
      "\n",
      "        [[[0.1004, 0.1004, 0.1004, 0.1005, 0.1006],\n",
      "          [0.1006, 0.1006, 0.1007, 0.1009, 0.1009],\n",
      "          [0.1013, 0.1012, 0.1013, 0.1011, 0.1009],\n",
      "          [0.1019, 0.1016, 0.1015, 0.1012, 0.1009],\n",
      "          [0.1018, 0.1014, 0.1016, 0.1016, 0.1013]]],\n",
      "\n",
      "\n",
      "        [[[0.1028, 0.1031, 0.1037, 0.1034, 0.1030],\n",
      "          [0.1014, 0.1011, 0.1009, 0.1007, 0.1011],\n",
      "          [0.1004, 0.1002, 0.1002, 0.1002, 0.1008],\n",
      "          [0.1002, 0.1002, 0.1003, 0.1007, 0.1025],\n",
      "          [0.1008, 0.1009, 0.1016, 0.1030, 0.1047]]],\n",
      "\n",
      "\n",
      "        [[[0.1010, 0.1009, 0.1007, 0.1006, 0.1008],\n",
      "          [0.1011, 0.1009, 0.1007, 0.1008, 0.1012],\n",
      "          [0.1012, 0.1013, 0.1012, 0.1015, 0.1016],\n",
      "          [0.1011, 0.1015, 0.1017, 0.1018, 0.1015],\n",
      "          [0.1009, 0.1013, 0.1017, 0.1016, 0.1011]]],\n",
      "\n",
      "\n",
      "        [[[0.1006, 0.1003, 0.1002, 0.1002, 0.1004],\n",
      "          [0.1009, 0.1004, 0.1002, 0.1002, 0.1004],\n",
      "          [0.1019, 0.1013, 0.1008, 0.1007, 0.1012],\n",
      "          [0.1036, 0.1036, 0.1031, 0.1031, 0.1037],\n",
      "          [0.1046, 0.1056, 0.1055, 0.1055, 0.1062]]],\n",
      "\n",
      "\n",
      "        [[[0.1011, 0.1011, 0.1010, 0.1010, 0.1010],\n",
      "          [0.1011, 0.1010, 0.1008, 0.1008, 0.1009],\n",
      "          [0.1008, 0.1007, 0.1005, 0.1004, 0.1006],\n",
      "          [0.1007, 0.1005, 0.1003, 0.1002, 0.1003],\n",
      "          [0.1006, 0.1004, 0.1002, 0.1002, 0.1003]]],\n",
      "\n",
      "\n",
      "        [[[0.1007, 0.1009, 0.1011, 0.1011, 0.1008],\n",
      "          [0.1008, 0.1007, 0.1009, 0.1011, 0.1008],\n",
      "          [0.1006, 0.1004, 0.1006, 0.1010, 0.1009],\n",
      "          [0.1003, 0.1003, 0.1006, 0.1010, 0.1010],\n",
      "          [0.1003, 0.1004, 0.1007, 0.1010, 0.1010]]],\n",
      "\n",
      "\n",
      "        [[[0.1006, 0.1003, 0.1004, 0.1012, 0.1030],\n",
      "          [0.1003, 0.1001, 0.1003, 0.1017, 0.1048],\n",
      "          [0.1001, 0.1001, 0.1008, 0.1036, 0.1070],\n",
      "          [0.1003, 0.1006, 0.1024, 0.1063, 0.1081],\n",
      "          [0.1010, 0.1021, 0.1049, 0.1082, 0.1082]]],\n",
      "\n",
      "\n",
      "        [[[0.1007, 0.1006, 0.1004, 0.1004, 0.1004],\n",
      "          [0.1008, 0.1008, 0.1005, 0.1005, 0.1005],\n",
      "          [0.1010, 0.1011, 0.1009, 0.1008, 0.1008],\n",
      "          [0.1010, 0.1012, 0.1012, 0.1012, 0.1010],\n",
      "          [0.1009, 0.1010, 0.1013, 0.1012, 0.1009]]],\n",
      "\n",
      "\n",
      "        [[[0.1003, 0.1002, 0.1002, 0.1003, 0.1004],\n",
      "          [0.1003, 0.1002, 0.1003, 0.1004, 0.1004],\n",
      "          [0.1003, 0.1003, 0.1004, 0.1004, 0.1005],\n",
      "          [0.1004, 0.1004, 0.1005, 0.1006, 0.1006],\n",
      "          [0.1007, 0.1007, 0.1007, 0.1006, 0.1005]]],\n",
      "\n",
      "\n",
      "        [[[0.1008, 0.1005, 0.1005, 0.1008, 0.1011],\n",
      "          [0.1007, 0.1005, 0.1006, 0.1012, 0.1014],\n",
      "          [0.1009, 0.1009, 0.1013, 0.1019, 0.1016],\n",
      "          [0.1013, 0.1017, 0.1021, 0.1020, 0.1013],\n",
      "          [0.1016, 0.1020, 0.1020, 0.1014, 0.1009]]],\n",
      "\n",
      "\n",
      "        [[[0.1003, 0.1003, 0.1003, 0.1004, 0.1008],\n",
      "          [0.1002, 0.1001, 0.1001, 0.1002, 0.1006],\n",
      "          [0.1004, 0.1003, 0.1003, 0.1005, 0.1011],\n",
      "          [0.1011, 0.1012, 0.1013, 0.1019, 0.1024],\n",
      "          [0.1018, 0.1023, 0.1026, 0.1032, 0.1034]]],\n",
      "\n",
      "\n",
      "        [[[0.1006, 0.1006, 0.1007, 0.1009, 0.1011],\n",
      "          [0.1004, 0.1004, 0.1005, 0.1009, 0.1012],\n",
      "          [0.1003, 0.1003, 0.1005, 0.1010, 0.1011],\n",
      "          [0.1003, 0.1004, 0.1008, 0.1011, 0.1010],\n",
      "          [0.1005, 0.1007, 0.1010, 0.1011, 0.1008]]],\n",
      "\n",
      "\n",
      "        [[[0.1077, 0.1128, 0.1163, 0.1178, 0.1155],\n",
      "          [0.1112, 0.1164, 0.1168, 0.1153, 0.1134],\n",
      "          [0.1129, 0.1133, 0.1097, 0.1080, 0.1084],\n",
      "          [0.1089, 0.1057, 0.1040, 0.1046, 0.1063],\n",
      "          [0.1051, 0.1029, 0.1029, 0.1048, 0.1079]]],\n",
      "\n",
      "\n",
      "        [[[0.1007, 0.1008, 0.1012, 0.1013, 0.1011],\n",
      "          [0.1008, 0.1010, 0.1014, 0.1018, 0.1015],\n",
      "          [0.1010, 0.1011, 0.1013, 0.1015, 0.1015],\n",
      "          [0.1009, 0.1008, 0.1007, 0.1011, 0.1013],\n",
      "          [0.1006, 0.1005, 0.1006, 0.1011, 0.1012]]],\n",
      "\n",
      "\n",
      "        [[[0.1003, 0.1004, 0.1010, 0.1026, 0.1041],\n",
      "          [0.1001, 0.1002, 0.1009, 0.1034, 0.1054],\n",
      "          [0.1002, 0.1003, 0.1011, 0.1044, 0.1061],\n",
      "          [0.1003, 0.1006, 0.1018, 0.1049, 0.1063],\n",
      "          [0.1006, 0.1011, 0.1026, 0.1050, 0.1054]]],\n",
      "\n",
      "\n",
      "        [[[0.1003, 0.1006, 0.1018, 0.1033, 0.1038],\n",
      "          [0.1002, 0.1006, 0.1026, 0.1052, 0.1048],\n",
      "          [0.1003, 0.1010, 0.1043, 0.1067, 0.1049],\n",
      "          [0.1007, 0.1024, 0.1069, 0.1076, 0.1042],\n",
      "          [0.1019, 0.1048, 0.1077, 0.1065, 0.1035]]],\n",
      "\n",
      "\n",
      "        [[[0.1011, 0.1008, 0.1008, 0.1010, 0.1011],\n",
      "          [0.1006, 0.1004, 0.1005, 0.1009, 0.1013],\n",
      "          [0.1004, 0.1004, 0.1006, 0.1012, 0.1013],\n",
      "          [0.1006, 0.1008, 0.1013, 0.1015, 0.1010],\n",
      "          [0.1010, 0.1014, 0.1016, 0.1012, 0.1007]]],\n",
      "\n",
      "\n",
      "        [[[0.1018, 0.1017, 0.1010, 0.1008, 0.1009],\n",
      "          [0.1017, 0.1013, 0.1008, 0.1008, 0.1010],\n",
      "          [0.1016, 0.1013, 0.1010, 0.1011, 0.1013],\n",
      "          [0.1014, 0.1016, 0.1016, 0.1017, 0.1017],\n",
      "          [0.1013, 0.1017, 0.1019, 0.1018, 0.1017]]],\n",
      "\n",
      "\n",
      "        [[[0.1008, 0.1007, 0.1004, 0.1002, 0.1003],\n",
      "          [0.1008, 0.1007, 0.1004, 0.1002, 0.1003],\n",
      "          [0.1008, 0.1008, 0.1005, 0.1004, 0.1006],\n",
      "          [0.1008, 0.1009, 0.1007, 0.1007, 0.1007],\n",
      "          [0.1007, 0.1009, 0.1009, 0.1008, 0.1007]]],\n",
      "\n",
      "\n",
      "        [[[0.1021, 0.1014, 0.1008, 0.1011, 0.1020],\n",
      "          [0.1020, 0.1011, 0.1009, 0.1019, 0.1030],\n",
      "          [0.1021, 0.1016, 0.1021, 0.1034, 0.1034],\n",
      "          [0.1025, 0.1027, 0.1033, 0.1034, 0.1024],\n",
      "          [0.1023, 0.1028, 0.1027, 0.1023, 0.1019]]],\n",
      "\n",
      "\n",
      "        [[[0.1049, 0.1054, 0.1058, 0.1066, 0.1072],\n",
      "          [0.1038, 0.1038, 0.1038, 0.1038, 0.1047],\n",
      "          [0.1024, 0.1020, 0.1014, 0.1015, 0.1028],\n",
      "          [0.1021, 0.1012, 0.1006, 0.1010, 0.1024],\n",
      "          [0.1030, 0.1017, 0.1010, 0.1015, 0.1027]]],\n",
      "\n",
      "\n",
      "        [[[0.1006, 0.1008, 0.1015, 0.1028, 0.1035],\n",
      "          [0.1005, 0.1007, 0.1015, 0.1032, 0.1041],\n",
      "          [0.1004, 0.1006, 0.1017, 0.1036, 0.1049],\n",
      "          [0.1004, 0.1008, 0.1021, 0.1042, 0.1053],\n",
      "          [0.1005, 0.1011, 0.1027, 0.1046, 0.1051]]]]), tensor([0.1016, 0.1010, 0.1082, 0.1039, 0.1124, 0.1033, 0.1021, 0.1010, 0.1231,\n",
      "        0.1040, 0.1052, 0.1279, 0.1028, 0.1297, 0.1011, 0.1028, 0.1171, 0.1025,\n",
      "        0.1013, 0.1041, 0.1127, 0.1018, 0.1424, 0.1034, 0.1111, 0.1158, 0.1041,\n",
      "        0.1024, 0.1013, 0.1079, 0.1206, 0.1106]), tensor([[[[0.1001, 0.1002, 0.1003, 0.1003, 0.1003],\n",
      "          [0.1001, 0.1002, 0.1004, 0.1004, 0.1003],\n",
      "          [0.1002, 0.1002, 0.1004, 0.1005, 0.1004],\n",
      "          [0.1002, 0.1002, 0.1003, 0.1005, 0.1005],\n",
      "          [0.1002, 0.1003, 0.1003, 0.1004, 0.1004]],\n",
      "\n",
      "         [[0.1001, 0.1002, 0.1002, 0.1002, 0.1001],\n",
      "          [0.1001, 0.1001, 0.1002, 0.1003, 0.1002],\n",
      "          [0.1001, 0.1001, 0.1002, 0.1003, 0.1002],\n",
      "          [0.1001, 0.1001, 0.1001, 0.1002, 0.1002],\n",
      "          [0.1001, 0.1001, 0.1001, 0.1001, 0.1001]],\n",
      "\n",
      "         [[0.1001, 0.1001, 0.1002, 0.1004, 0.1003],\n",
      "          [0.1001, 0.1002, 0.1002, 0.1004, 0.1004],\n",
      "          [0.1002, 0.1003, 0.1002, 0.1003, 0.1004],\n",
      "          [0.1002, 0.1002, 0.1002, 0.1003, 0.1004],\n",
      "          [0.1001, 0.1001, 0.1002, 0.1003, 0.1003]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1001, 0.1001, 0.1001, 0.1002, 0.1001],\n",
      "          [0.1001, 0.1001, 0.1001, 0.1002, 0.1002],\n",
      "          [0.1001, 0.1001, 0.1001, 0.1001, 0.1002],\n",
      "          [0.1001, 0.1001, 0.1001, 0.1001, 0.1002],\n",
      "          [0.1001, 0.1001, 0.1001, 0.1001, 0.1001]],\n",
      "\n",
      "         [[0.1001, 0.1001, 0.1001, 0.1001, 0.1001],\n",
      "          [0.1002, 0.1003, 0.1001, 0.1001, 0.1001],\n",
      "          [0.1001, 0.1002, 0.1003, 0.1001, 0.1000],\n",
      "          [0.1001, 0.1002, 0.1003, 0.1002, 0.1001],\n",
      "          [0.1002, 0.1002, 0.1004, 0.1005, 0.1004]],\n",
      "\n",
      "         [[0.1001, 0.1002, 0.1002, 0.1001, 0.1001],\n",
      "          [0.1001, 0.1002, 0.1003, 0.1001, 0.1000],\n",
      "          [0.1001, 0.1002, 0.1003, 0.1002, 0.1000],\n",
      "          [0.1001, 0.1001, 0.1003, 0.1002, 0.1001],\n",
      "          [0.1001, 0.1001, 0.1002, 0.1002, 0.1001]]],\n",
      "\n",
      "\n",
      "        [[[0.1005, 0.1005, 0.1003, 0.1002, 0.1003],\n",
      "          [0.1006, 0.1003, 0.1002, 0.1003, 0.1005],\n",
      "          [0.1006, 0.1003, 0.1003, 0.1007, 0.1010],\n",
      "          [0.1007, 0.1005, 0.1007, 0.1012, 0.1014],\n",
      "          [0.1008, 0.1008, 0.1011, 0.1015, 0.1013]],\n",
      "\n",
      "         [[0.1003, 0.1001, 0.1001, 0.1001, 0.1002],\n",
      "          [0.1003, 0.1002, 0.1002, 0.1003, 0.1004],\n",
      "          [0.1004, 0.1003, 0.1004, 0.1006, 0.1006],\n",
      "          [0.1005, 0.1005, 0.1005, 0.1006, 0.1006],\n",
      "          [0.1004, 0.1005, 0.1005, 0.1006, 0.1005]],\n",
      "\n",
      "         [[0.1011, 0.1010, 0.1002, 0.1001, 0.1003],\n",
      "          [0.1012, 0.1005, 0.1001, 0.1002, 0.1009],\n",
      "          [0.1008, 0.1003, 0.1002, 0.1008, 0.1019],\n",
      "          [0.1005, 0.1004, 0.1007, 0.1017, 0.1018],\n",
      "          [0.1005, 0.1006, 0.1012, 0.1016, 0.1011]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1003, 0.1001, 0.1001, 0.1001, 0.1002],\n",
      "          [0.1002, 0.1001, 0.1001, 0.1003, 0.1005],\n",
      "          [0.1002, 0.1003, 0.1005, 0.1007, 0.1006],\n",
      "          [0.1003, 0.1006, 0.1007, 0.1006, 0.1004],\n",
      "          [0.1004, 0.1006, 0.1005, 0.1003, 0.1004]],\n",
      "\n",
      "         [[0.1002, 0.1003, 0.1003, 0.1003, 0.1002],\n",
      "          [0.1002, 0.1002, 0.1002, 0.1001, 0.1001],\n",
      "          [0.1002, 0.1002, 0.1002, 0.1001, 0.1001],\n",
      "          [0.1002, 0.1003, 0.1003, 0.1003, 0.1003],\n",
      "          [0.1006, 0.1006, 0.1008, 0.1007, 0.1005]],\n",
      "\n",
      "         [[0.1001, 0.1000, 0.1001, 0.1003, 0.1003],\n",
      "          [0.1001, 0.1001, 0.1003, 0.1005, 0.1003],\n",
      "          [0.1001, 0.1002, 0.1006, 0.1006, 0.1002],\n",
      "          [0.1002, 0.1004, 0.1008, 0.1006, 0.1001],\n",
      "          [0.1004, 0.1005, 0.1007, 0.1005, 0.1002]]],\n",
      "\n",
      "\n",
      "        [[[0.1006, 0.1006, 0.1005, 0.1004, 0.1005],\n",
      "          [0.1007, 0.1006, 0.1003, 0.1003, 0.1005],\n",
      "          [0.1008, 0.1006, 0.1003, 0.1003, 0.1006],\n",
      "          [0.1008, 0.1008, 0.1004, 0.1004, 0.1007],\n",
      "          [0.1007, 0.1007, 0.1005, 0.1003, 0.1007]],\n",
      "\n",
      "         [[0.1003, 0.1002, 0.1001, 0.1002, 0.1003],\n",
      "          [0.1004, 0.1002, 0.1001, 0.1001, 0.1004],\n",
      "          [0.1005, 0.1004, 0.1002, 0.1002, 0.1005],\n",
      "          [0.1004, 0.1004, 0.1003, 0.1002, 0.1004],\n",
      "          [0.1003, 0.1004, 0.1002, 0.1002, 0.1003]],\n",
      "\n",
      "         [[0.1008, 0.1011, 0.1005, 0.1001, 0.1003],\n",
      "          [0.1009, 0.1009, 0.1003, 0.1000, 0.1003],\n",
      "          [0.1007, 0.1007, 0.1003, 0.1001, 0.1004],\n",
      "          [0.1006, 0.1007, 0.1004, 0.1002, 0.1005],\n",
      "          [0.1005, 0.1006, 0.1003, 0.1002, 0.1006]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1003, 0.1002, 0.1001, 0.1001, 0.1001],\n",
      "          [0.1002, 0.1002, 0.1001, 0.1001, 0.1002],\n",
      "          [0.1002, 0.1003, 0.1003, 0.1003, 0.1003],\n",
      "          [0.1002, 0.1003, 0.1003, 0.1002, 0.1002],\n",
      "          [0.1001, 0.1002, 0.1002, 0.1002, 0.1002]],\n",
      "\n",
      "         [[0.1003, 0.1005, 0.1006, 0.1005, 0.1003],\n",
      "          [0.1003, 0.1004, 0.1005, 0.1005, 0.1002],\n",
      "          [0.1002, 0.1001, 0.1002, 0.1005, 0.1003],\n",
      "          [0.1004, 0.1003, 0.1004, 0.1007, 0.1006],\n",
      "          [0.1007, 0.1006, 0.1006, 0.1007, 0.1006]],\n",
      "\n",
      "         [[0.1003, 0.1001, 0.1001, 0.1003, 0.1003],\n",
      "          [0.1003, 0.1001, 0.1001, 0.1004, 0.1005],\n",
      "          [0.1003, 0.1001, 0.1001, 0.1004, 0.1006],\n",
      "          [0.1004, 0.1001, 0.1001, 0.1004, 0.1006],\n",
      "          [0.1004, 0.1002, 0.1001, 0.1004, 0.1005]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.1005, 0.1007, 0.1006, 0.1004, 0.1003],\n",
      "          [0.1005, 0.1007, 0.1006, 0.1004, 0.1003],\n",
      "          [0.1007, 0.1008, 0.1007, 0.1004, 0.1004],\n",
      "          [0.1010, 0.1011, 0.1009, 0.1006, 0.1004],\n",
      "          [0.1009, 0.1010, 0.1008, 0.1006, 0.1004]],\n",
      "\n",
      "         [[0.1002, 0.1004, 0.1003, 0.1002, 0.1002],\n",
      "          [0.1002, 0.1003, 0.1003, 0.1002, 0.1002],\n",
      "          [0.1005, 0.1005, 0.1004, 0.1003, 0.1002],\n",
      "          [0.1005, 0.1006, 0.1005, 0.1003, 0.1002],\n",
      "          [0.1002, 0.1003, 0.1003, 0.1002, 0.1002]],\n",
      "\n",
      "         [[0.1005, 0.1008, 0.1011, 0.1006, 0.1004],\n",
      "          [0.1005, 0.1012, 0.1010, 0.1005, 0.1005],\n",
      "          [0.1007, 0.1012, 0.1008, 0.1007, 0.1007],\n",
      "          [0.1009, 0.1010, 0.1009, 0.1009, 0.1006],\n",
      "          [0.1009, 0.1007, 0.1007, 0.1006, 0.1003]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1002, 0.1003, 0.1003, 0.1002, 0.1001],\n",
      "          [0.1003, 0.1002, 0.1002, 0.1002, 0.1002],\n",
      "          [0.1005, 0.1003, 0.1004, 0.1004, 0.1002],\n",
      "          [0.1004, 0.1004, 0.1004, 0.1003, 0.1001],\n",
      "          [0.1002, 0.1002, 0.1002, 0.1001, 0.1001]],\n",
      "\n",
      "         [[0.1005, 0.1003, 0.1002, 0.1002, 0.1002],\n",
      "          [0.1003, 0.1003, 0.1003, 0.1002, 0.1002],\n",
      "          [0.1001, 0.1001, 0.1001, 0.1001, 0.1001],\n",
      "          [0.1004, 0.1003, 0.1002, 0.1002, 0.1002],\n",
      "          [0.1009, 0.1008, 0.1006, 0.1004, 0.1003]],\n",
      "\n",
      "         [[0.1004, 0.1004, 0.1002, 0.1001, 0.1001],\n",
      "          [0.1005, 0.1003, 0.1002, 0.1002, 0.1001],\n",
      "          [0.1004, 0.1003, 0.1002, 0.1002, 0.1001],\n",
      "          [0.1005, 0.1003, 0.1002, 0.1001, 0.1001],\n",
      "          [0.1004, 0.1003, 0.1002, 0.1001, 0.1001]]],\n",
      "\n",
      "\n",
      "        [[[0.1001, 0.1001, 0.1001, 0.1001, 0.1001],\n",
      "          [0.1001, 0.1001, 0.1002, 0.1002, 0.1003],\n",
      "          [0.1001, 0.1002, 0.1002, 0.1003, 0.1003],\n",
      "          [0.1001, 0.1002, 0.1002, 0.1002, 0.1002],\n",
      "          [0.1001, 0.1002, 0.1002, 0.1002, 0.1001]],\n",
      "\n",
      "         [[0.1001, 0.1001, 0.1001, 0.1001, 0.1001],\n",
      "          [0.1001, 0.1001, 0.1002, 0.1002, 0.1002],\n",
      "          [0.1000, 0.1001, 0.1001, 0.1001, 0.1001],\n",
      "          [0.1000, 0.1001, 0.1001, 0.1001, 0.1001],\n",
      "          [0.1001, 0.1001, 0.1001, 0.1001, 0.1001]],\n",
      "\n",
      "         [[0.1000, 0.1001, 0.1001, 0.1001, 0.1002],\n",
      "          [0.1001, 0.1001, 0.1001, 0.1003, 0.1003],\n",
      "          [0.1001, 0.1002, 0.1003, 0.1004, 0.1002],\n",
      "          [0.1001, 0.1002, 0.1003, 0.1002, 0.1002],\n",
      "          [0.1001, 0.1001, 0.1002, 0.1002, 0.1002]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1001, 0.1001, 0.1001, 0.1002],\n",
      "          [0.1001, 0.1001, 0.1002, 0.1002, 0.1002],\n",
      "          [0.1001, 0.1001, 0.1001, 0.1001, 0.1000],\n",
      "          [0.1001, 0.1001, 0.1001, 0.1001, 0.1001],\n",
      "          [0.1000, 0.1001, 0.1001, 0.1001, 0.1001]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000, 0.1000, 0.1000],\n",
      "          [0.1001, 0.1001, 0.1001, 0.1001, 0.1001],\n",
      "          [0.1002, 0.1002, 0.1003, 0.1003, 0.1003],\n",
      "          [0.1002, 0.1002, 0.1002, 0.1002, 0.1002],\n",
      "          [0.1001, 0.1001, 0.1001, 0.1001, 0.1001]],\n",
      "\n",
      "         [[0.1001, 0.1001, 0.1001, 0.1001, 0.1000],\n",
      "          [0.1001, 0.1001, 0.1001, 0.1001, 0.1000],\n",
      "          [0.1001, 0.1001, 0.1001, 0.1001, 0.1001],\n",
      "          [0.1001, 0.1001, 0.1001, 0.1001, 0.1001],\n",
      "          [0.1001, 0.1001, 0.1001, 0.1001, 0.1001]]],\n",
      "\n",
      "\n",
      "        [[[0.1008, 0.1008, 0.1007, 0.1005, 0.1005],\n",
      "          [0.1011, 0.1011, 0.1008, 0.1005, 0.1005],\n",
      "          [0.1011, 0.1012, 0.1010, 0.1006, 0.1005],\n",
      "          [0.1011, 0.1012, 0.1010, 0.1006, 0.1004],\n",
      "          [0.1011, 0.1011, 0.1007, 0.1004, 0.1003]],\n",
      "\n",
      "         [[0.1005, 0.1005, 0.1004, 0.1003, 0.1003],\n",
      "          [0.1006, 0.1007, 0.1005, 0.1003, 0.1003],\n",
      "          [0.1005, 0.1006, 0.1005, 0.1003, 0.1002],\n",
      "          [0.1005, 0.1004, 0.1003, 0.1002, 0.1002],\n",
      "          [0.1003, 0.1003, 0.1002, 0.1001, 0.1002]],\n",
      "\n",
      "         [[0.1008, 0.1010, 0.1009, 0.1007, 0.1006],\n",
      "          [0.1008, 0.1008, 0.1012, 0.1010, 0.1008],\n",
      "          [0.1006, 0.1009, 0.1017, 0.1011, 0.1005],\n",
      "          [0.1007, 0.1018, 0.1016, 0.1006, 0.1003],\n",
      "          [0.1010, 0.1014, 0.1007, 0.1002, 0.1003]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1005, 0.1004, 0.1003, 0.1003, 0.1003],\n",
      "          [0.1004, 0.1004, 0.1005, 0.1004, 0.1003],\n",
      "          [0.1003, 0.1005, 0.1005, 0.1003, 0.1002],\n",
      "          [0.1006, 0.1005, 0.1003, 0.1002, 0.1002],\n",
      "          [0.1004, 0.1002, 0.1001, 0.1002, 0.1002]],\n",
      "\n",
      "         [[0.1003, 0.1003, 0.1003, 0.1003, 0.1002],\n",
      "          [0.1004, 0.1003, 0.1004, 0.1004, 0.1004],\n",
      "          [0.1006, 0.1004, 0.1003, 0.1004, 0.1004],\n",
      "          [0.1005, 0.1005, 0.1004, 0.1004, 0.1003],\n",
      "          [0.1009, 0.1009, 0.1007, 0.1004, 0.1003]],\n",
      "\n",
      "         [[0.1003, 0.1003, 0.1002, 0.1003, 0.1002],\n",
      "          [0.1004, 0.1003, 0.1003, 0.1002, 0.1002],\n",
      "          [0.1005, 0.1004, 0.1002, 0.1001, 0.1003],\n",
      "          [0.1005, 0.1004, 0.1001, 0.1001, 0.1003],\n",
      "          [0.1004, 0.1003, 0.1001, 0.1002, 0.1003]]]]), tensor([0.1005, 0.1023, 0.1019, 0.1006, 0.1007, 0.1005, 0.1000, 0.1005, 0.1013,\n",
      "        0.1026, 0.1009, 0.1003, 0.1007, 0.1004, 0.1004, 0.1008, 0.1011, 0.1008,\n",
      "        0.1007, 0.1005, 0.1000, 0.1010, 0.1000, 0.1008, 0.1013, 0.1007, 0.1010,\n",
      "        0.1015, 0.1012, 0.1010, 0.1004, 0.1012]), tensor([[0.1003, 0.1012, 0.1039,  ..., 0.1039, 0.1156, 0.1106],\n",
      "        [0.1008, 0.1029, 0.1018,  ..., 0.1071, 0.1087, 0.1041],\n",
      "        [0.1007, 0.1043, 0.1093,  ..., 0.1119, 0.1186, 0.1132],\n",
      "        ...,\n",
      "        [0.1007, 0.1028, 0.1044,  ..., 0.1165, 0.1201, 0.1071],\n",
      "        [0.1018, 0.1035, 0.1086,  ..., 0.1104, 0.1327, 0.1374],\n",
      "        [0.1015, 0.1036, 0.1092,  ..., 0.1207, 0.1329, 0.1156]]), tensor([0.1015, 0.1017, 0.1029, 0.1028, 0.1025, 0.1022, 0.1015, 0.1025, 0.1032,\n",
      "        0.1036])]\n"
     ]
    }
   ],
   "source": [
    "#print(MNIST_NN_Hessian_diag_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST_NN_Hessian_diag_60 = get_Hessian_NN(model=mnist_model, train_loader=mnist_train_loader, prec0=60,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(MNIST_NN_Hessian_diag_60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_NN_Hessian_diag_120 = get_Hessian_NN(model=mnist_model, train_loader=mnist_train_loader, prec0=120,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(MNIST_NN_Hessian_diag_120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_NN_Hessian_diag_1000 = get_Hessian_NN(model=mnist_model, train_loader=mnist_train_loader, prec0=1000,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(MNIST_NN_Hessian_diag_1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As you can see, the variance gets smaller, the higher the precision gets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we want to look at the single layers of our network, and how they behave w.r.t. the variance\n",
    "* every tensor represents one of the six layers of out network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meancalc(Hessian_diag_x): \n",
    "    for ith_tensor in range(len(Hessian_diag_x)) :\n",
    "        mean = torch.mean(Hessian_diag_x[ith_tensor])\n",
    "        print(\"mean variance of layer {0:d}: {1:.4f}\".format(ith_tensor+1, mean.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print(\"precision 10:\")\n",
    "meancalc(MNIST_NN_Hessian_diag_10)\n",
    "print(\"precision 60:\")\n",
    "meancalc(MNIST_NN_Hessian_diag_60)\n",
    "print(\"precision 120:\")\n",
    "meancalc(MNIST_NN_Hessian_diag_120)\n",
    "print(\"precision 1000:\")\n",
    "meancalc(MNIST_NN_Hessian_diag_1000)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### it's not save to say, that the variance gets smaller in deeper parts of this network\n",
    "* it's clear, that the flatten-layer has a bigger variance, than the previous layers\n",
    "* after the flatten-layer it drops in all of the observations\n",
    "* also in all cases the variance in the first layer is smaller, than in the fc-layer\n",
    "* the first maxpool layer has the biggest variance in all obsersations - obviously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom PIL import Image\\nyticks = np.arange(10, 0, step=1)\\nxticks = np.arange(0, 512, step=100)\\na = np.array(MNIST_NN_Hessian_diag_10[4])\\nplt.yticks = yticks\\nplt.xticks = xticks\\nplt.plot(a)\\nplt.show()\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from PIL import Image\n",
    "yticks = np.arange(10, 0, step=1)\n",
    "xticks = np.arange(0, 512, step=100)\n",
    "a = np.array(MNIST_NN_Hessian_diag_10[4])\n",
    "plt.yticks = yticks\n",
    "plt.xticks = xticks\n",
    "plt.plot(a)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import seaborn as sns\n",
    "test = MNIST_NN_Hessian_diag_10[0][0]\n",
    "test2 = MNIST_NN_Hessian_diag_10[0][1]\n",
    "testit = np.concatenate((test, test2))\n",
    "testittp = testit.transpose(2, 0, 1).reshape(5, -1)\n",
    "arr = np.array(testittp)\n",
    "print(testit)\n",
    "print(\"---\")\n",
    "print(arr)\n",
    "ax = sns.heatmap(arr, linewidth=0.5)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize the first layer of our networks in a heatmap\n",
    "* therefore we put the tensor in the right form/dimensions, by concatening all of its included arrays and then reshaping the tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "def visualize(tensor):\n",
    "    output = tensor[0][0]\n",
    "    for i in range(1, len(tensor[0])):\n",
    "        output = np.concatenate((output, tensor[0][i]))\n",
    "    output = output.transpose(2, 0, 1).reshape(5, -1)\n",
    "    heatmap = sns.heatmap(output)\n",
    "    plt.xticks = (np.arange(0, step=20))\n",
    "    plt.show()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize(MNIST_NN_Hessian_diag_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meancalc2(Hessian_diag_x):\n",
    "    i = 0 \n",
    "    for name, parameter in mnist_model.named_parameters():\n",
    "        mean = torch.mean(Hessian_diag_x[i])\n",
    "        print(\"mean variance of layer {0:s}: {1:.4f}\".format(name, mean.item()))\n",
    "        i += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean variance of layer 0.weight: 0.1018\n",
      "mean variance of layer 0.bias: 0.1090\n",
      "mean variance of layer 3.weight: 0.1002\n",
      "mean variance of layer 3.bias: 0.1009\n",
      "mean variance of layer 7.weight: 0.1085\n",
      "mean variance of layer 7.bias: 0.1024\n"
     ]
    }
   ],
   "source": [
    "#meancalc2(Hessian_diag_x=MNIST_NN_Hessian_diag_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "1 ReLU()\n",
      "2 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "3 Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "4 ReLU()\n",
      "5 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "6 Flatten()\n",
      "7 Linear(in_features=512, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Parameter containing:\n",
      "tensor([-0.0228,  0.0271, -0.0055,  0.0152, -0.0224, -0.0110,  0.0214, -0.0176,\n",
      "         0.0134,  0.0330], requires_grad=True)\n",
      "0 Parameter containing:\n",
      "tensor([-0.0228,  0.0271, -0.0055,  0.0152, -0.0224, -0.0110,  0.0214, -0.0176,\n",
      "         0.0134,  0.0330], requires_grad=True)\n",
      "1 Parameter containing:\n",
      "tensor([-0.0228,  0.0271, -0.0055,  0.0152, -0.0224, -0.0110,  0.0214, -0.0176,\n",
      "         0.0134,  0.0330], requires_grad=True)\n",
      "2 Parameter containing:\n",
      "tensor([-0.0228,  0.0271, -0.0055,  0.0152, -0.0224, -0.0110,  0.0214, -0.0176,\n",
      "         0.0134,  0.0330], requires_grad=True)\n",
      "3 Parameter containing:\n",
      "tensor([-0.0228,  0.0271, -0.0055,  0.0152, -0.0224, -0.0110,  0.0214, -0.0176,\n",
      "         0.0134,  0.0330], requires_grad=True)\n",
      "4 Parameter containing:\n",
      "tensor([-0.0228,  0.0271, -0.0055,  0.0152, -0.0224, -0.0110,  0.0214, -0.0176,\n",
      "         0.0134,  0.0330], requires_grad=True)\n",
      "5 Parameter containing:\n",
      "tensor([-0.0228,  0.0271, -0.0055,  0.0152, -0.0224, -0.0110,  0.0214, -0.0176,\n",
      "         0.0134,  0.0330], requires_grad=True)\n",
      "6 Parameter containing:\n",
      "tensor([-0.0228,  0.0271, -0.0055,  0.0152, -0.0224, -0.0110,  0.0214, -0.0176,\n",
      "         0.0134,  0.0330], requires_grad=True)\n",
      "7 Parameter containing:\n",
      "tensor([-0.0228,  0.0271, -0.0055,  0.0152, -0.0224, -0.0110,  0.0214, -0.0176,\n",
      "         0.0134,  0.0330], requires_grad=True)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['0.weight', '0.bias', '3.weight', '3.bias', '7.weight', '7.bias'])\n",
      "tensor([[[[-1.1223e-01,  6.0851e-02,  1.7020e-01,  1.4795e-01, -8.1844e-02],\n",
      "          [-6.6769e-02, -1.0490e-01,  6.6525e-02, -1.1420e-01,  1.0781e-01],\n",
      "          [ 1.9580e-01, -1.0139e-01,  1.5808e-01, -3.9838e-02,  1.9428e-01],\n",
      "          [-1.6455e-01,  2.0796e-02,  1.4474e-01, -1.9619e-01,  2.3047e-02],\n",
      "          [-8.5486e-02,  1.1187e-01,  1.8013e-01, -7.2583e-02,  3.3703e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.6629e-01,  2.4010e-02, -1.7210e-01,  1.5694e-01,  1.7176e-01],\n",
      "          [-1.0692e-01, -1.2006e-01, -1.0162e-01, -8.3380e-02, -1.2339e-01],\n",
      "          [ 1.3451e-01, -1.6791e-01, -1.6872e-01, -6.3949e-02, -1.9162e-01],\n",
      "          [-9.6071e-02,  4.4107e-02, -1.6081e-01, -4.8291e-02, -1.6280e-01],\n",
      "          [-1.6614e-01, -1.5058e-01, -1.3164e-01,  3.5934e-02,  3.2687e-02]]],\n",
      "\n",
      "\n",
      "        [[[-8.3437e-02, -1.6796e-01,  1.2640e-01,  1.8091e-02, -2.3688e-02],\n",
      "          [-1.8711e-01,  1.1519e-01,  1.1407e-01,  8.8518e-02,  4.5294e-02],\n",
      "          [-6.9150e-02,  1.8383e-01, -1.3836e-02, -7.5586e-02,  6.8938e-02],\n",
      "          [-6.6491e-02,  1.9604e-02, -3.8867e-02,  9.2996e-02, -4.0108e-02],\n",
      "          [-1.9646e-01,  2.5916e-02, -1.2875e-01, -2.0166e-02, -1.0500e-02]]],\n",
      "\n",
      "\n",
      "        [[[-5.5196e-03, -1.1795e-01,  4.3320e-02,  1.8070e-02,  2.3354e-02],\n",
      "          [ 5.8653e-02, -1.9547e-01,  1.0230e-01, -5.5797e-02, -3.9502e-02],\n",
      "          [ 4.6874e-02,  4.5285e-02, -6.2303e-02,  3.8205e-03, -1.4295e-01],\n",
      "          [-7.9965e-02, -6.9738e-02,  2.9297e-02,  3.9158e-02, -5.7600e-02],\n",
      "          [-5.9063e-02,  1.4223e-01,  1.3826e-02, -1.6133e-02, -1.6479e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.6905e-01, -1.6879e-01, -1.0352e-03, -1.8403e-01, -1.6469e-01],\n",
      "          [ 2.7154e-02, -8.7234e-02, -1.4099e-01, -1.0867e-01, -1.3482e-01],\n",
      "          [ 1.2596e-01,  6.7098e-02, -1.5251e-01,  1.3728e-01,  1.3994e-01],\n",
      "          [-1.8459e-01,  1.3808e-02, -2.1086e-02, -1.7988e-01, -1.4153e-01],\n",
      "          [ 1.4655e-02,  1.0409e-01,  1.8831e-01, -2.5071e-02,  8.3483e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9936e-01,  1.0655e-01, -1.3873e-01,  1.2178e-01,  3.2062e-02],\n",
      "          [ 8.1163e-02,  1.8109e-01, -1.8438e-01,  1.1245e-02,  1.6837e-01],\n",
      "          [ 9.1524e-02,  5.5727e-02,  4.5322e-02,  1.1519e-01, -3.0628e-02],\n",
      "          [ 9.2590e-02,  1.5096e-01, -1.2440e-01,  1.4145e-01,  5.1670e-02],\n",
      "          [-5.4572e-02,  7.0935e-02, -1.4419e-01,  1.8826e-01,  2.2261e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 5.0957e-02, -1.2523e-01,  1.1814e-01, -1.8264e-01, -1.3178e-01],\n",
      "          [ 1.2286e-01, -9.6034e-02,  3.1380e-02, -3.7502e-02,  1.4315e-01],\n",
      "          [ 1.3068e-01, -1.4546e-01, -7.8739e-02,  1.4239e-01, -1.1135e-02],\n",
      "          [-1.4722e-01, -1.8683e-01, -1.4291e-01,  7.3742e-02,  8.8639e-02],\n",
      "          [ 1.4419e-01,  1.2737e-01, -1.4356e-02,  6.9339e-02, -1.5106e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0650e-01,  7.0679e-02, -1.5458e-01, -9.3370e-02, -2.4422e-03],\n",
      "          [-5.5671e-02,  4.2583e-02, -1.3580e-01, -2.7678e-02,  1.0756e-04],\n",
      "          [ 1.4838e-01,  1.9201e-01,  1.5339e-01,  1.9211e-01,  1.7150e-01],\n",
      "          [ 1.7346e-01, -3.2195e-02, -1.1293e-01,  5.7536e-02, -1.2609e-01],\n",
      "          [-6.6397e-02,  7.9204e-02,  3.1711e-02, -1.1999e-01, -1.5901e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.3271e-03, -1.8521e-01, -3.1861e-02, -1.0043e-01,  6.5732e-02],\n",
      "          [ 2.9330e-03,  5.6472e-02, -9.1569e-02,  1.3630e-01,  7.0146e-03],\n",
      "          [-1.8145e-01, -1.0281e-01, -1.0701e-01, -5.1833e-02,  1.7473e-01],\n",
      "          [ 1.3228e-01, -1.4569e-01, -1.4279e-01, -9.4916e-02, -1.5275e-01],\n",
      "          [-2.1326e-02,  1.6732e-01,  2.9304e-02, -1.5687e-01, -1.5710e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.1306e-02, -7.7857e-02, -1.6968e-02, -2.3156e-02, -2.1082e-02],\n",
      "          [-6.4454e-02, -7.5282e-02,  1.0515e-01, -1.4814e-01,  5.4288e-02],\n",
      "          [ 1.3732e-01, -1.5983e-01, -1.2142e-01,  4.6151e-02,  1.2427e-01],\n",
      "          [ 1.2681e-01,  3.2639e-02, -1.4349e-01,  9.1380e-03, -1.6549e-01],\n",
      "          [ 1.3972e-01, -1.4869e-01, -9.2185e-02, -9.2436e-03, -1.5729e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4500e-01, -1.6459e-01, -4.0202e-02, -1.2147e-01,  6.3511e-02],\n",
      "          [ 1.9576e-01, -6.8880e-02, -1.0485e-01,  3.4027e-02, -1.9794e-01],\n",
      "          [ 6.6472e-02, -7.4165e-02,  1.6409e-01,  1.8704e-01, -1.8954e-01],\n",
      "          [ 1.8137e-01,  5.4881e-02, -1.4985e-01, -1.6733e-01,  1.1623e-01],\n",
      "          [-9.3810e-02,  1.4954e-01, -1.0406e-01, -5.1225e-02, -1.9192e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1760e-01, -9.6733e-02, -2.3457e-02, -1.3038e-01, -1.4622e-01],\n",
      "          [ 1.8166e-01,  7.6937e-03, -1.7774e-01, -3.4263e-03, -1.8148e-01],\n",
      "          [-5.8933e-02, -1.8389e-01,  1.2400e-01, -2.2637e-02, -1.1924e-01],\n",
      "          [-1.3888e-01, -1.2672e-01,  1.2376e-01, -1.0360e-01,  7.0617e-04],\n",
      "          [-7.0792e-02, -7.4268e-02,  1.9026e-01, -1.0181e-01, -6.2322e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.4766e-02, -7.0993e-02,  3.0605e-02, -1.9578e-01, -3.3487e-02],\n",
      "          [-1.9099e-01, -1.3534e-01,  6.5075e-02,  1.6790e-02, -1.5028e-01],\n",
      "          [-8.7692e-02,  4.3355e-02,  1.5682e-01,  5.3391e-02, -1.1719e-01],\n",
      "          [-2.7624e-02,  5.6477e-02, -1.7685e-01, -1.5294e-01, -8.5193e-02],\n",
      "          [-1.8362e-02, -6.0614e-02,  2.2887e-02,  1.1001e-01, -1.3464e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.5437e-01,  1.9000e-01,  1.4790e-01, -3.0276e-02, -1.9513e-01],\n",
      "          [ 1.6252e-01, -6.5354e-02, -1.7157e-01,  3.7903e-03, -1.3497e-01],\n",
      "          [ 5.8029e-02, -1.5307e-01,  1.6251e-01, -1.4211e-02, -2.4664e-02],\n",
      "          [ 1.1634e-01,  1.2950e-01,  7.3606e-02,  1.5641e-01,  8.6034e-02],\n",
      "          [-3.7497e-02, -1.5556e-01, -3.5699e-03, -4.8753e-02,  1.8720e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.6407e-01,  1.2950e-02, -1.5521e-01,  1.5436e-01,  3.8417e-02],\n",
      "          [ 1.1625e-03,  1.1241e-02,  1.9501e-01,  3.3886e-03, -2.2523e-02],\n",
      "          [-4.9673e-02,  4.8728e-03, -1.1803e-01,  7.9630e-02,  2.3704e-02],\n",
      "          [-5.6621e-02, -1.7642e-01,  9.9463e-02,  9.1029e-02, -1.8918e-01],\n",
      "          [ 2.2912e-02, -1.9021e-01, -1.2828e-01,  1.2625e-01,  7.3675e-02]]],\n",
      "\n",
      "\n",
      "        [[[-8.0090e-02,  1.7927e-01,  1.4496e-01,  1.9951e-02,  2.9362e-02],\n",
      "          [-4.2177e-02, -1.0362e-01, -9.4902e-03,  4.5576e-02, -4.6871e-02],\n",
      "          [-1.2503e-03, -1.7307e-03, -1.9937e-01,  5.3825e-02,  1.7184e-01],\n",
      "          [ 8.2614e-03, -5.4727e-02,  1.6707e-01,  2.4318e-02, -4.4966e-02],\n",
      "          [-1.9891e-01, -1.7312e-01, -2.6726e-03,  8.9481e-02,  5.8464e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.5622e-01, -4.7386e-02,  9.4765e-03,  1.8958e-01,  1.5939e-01],\n",
      "          [ 7.2839e-02,  9.0757e-02, -1.0113e-01,  8.4511e-02, -6.6943e-02],\n",
      "          [-1.4963e-01,  1.6722e-02, -1.0661e-01, -1.0609e-01,  4.0627e-02],\n",
      "          [-4.1120e-02,  2.7285e-02,  7.2967e-02, -1.6683e-01,  1.0144e-01],\n",
      "          [-7.1005e-02,  9.5207e-02, -7.4462e-02, -1.1778e-01,  1.2358e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.0567e-01,  5.2470e-02, -1.1193e-01,  3.4212e-03,  1.5302e-01],\n",
      "          [ 1.1704e-01,  1.3577e-01, -8.7723e-02, -1.6455e-01, -1.9550e-01],\n",
      "          [-1.5167e-01,  8.8554e-02,  1.8142e-01,  1.2683e-01, -1.5956e-02],\n",
      "          [ 1.6862e-01,  9.9380e-02,  3.7532e-02,  8.2440e-02, -3.2711e-02],\n",
      "          [-8.0530e-02,  6.7764e-02, -4.6616e-02, -1.0902e-02,  1.5991e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1174e-01, -1.7809e-01, -9.3716e-02,  1.8587e-01,  5.1369e-03],\n",
      "          [-7.4896e-02,  1.7198e-01,  8.8109e-02,  1.5378e-01,  1.1247e-01],\n",
      "          [-1.2544e-01,  1.9765e-01,  1.9712e-01, -3.3413e-02, -9.5115e-03],\n",
      "          [ 1.6672e-01,  1.7602e-01,  8.3708e-02,  4.1403e-02, -9.1564e-02],\n",
      "          [-1.3261e-01,  4.9248e-02,  1.8855e-01, -2.0102e-02,  1.3827e-01]]],\n",
      "\n",
      "\n",
      "        [[[-5.1192e-02,  1.7010e-01,  2.6160e-02, -1.0951e-01,  1.4213e-01],\n",
      "          [ 1.7736e-01, -1.8733e-01, -1.9975e-01, -1.2225e-01,  7.2892e-02],\n",
      "          [ 1.8859e-01,  1.4403e-01,  1.1276e-02,  3.3252e-02,  4.0184e-02],\n",
      "          [ 6.7426e-02, -4.5968e-02,  4.6555e-02, -1.6504e-01, -1.4004e-01],\n",
      "          [-9.8262e-02, -1.5152e-01,  1.5646e-01,  5.1426e-02,  1.3590e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.6678e-01,  8.4061e-02,  1.1858e-01,  1.5424e-01, -1.2583e-01],\n",
      "          [-9.2135e-02, -3.1182e-02, -9.8257e-02, -1.3172e-01, -1.8864e-01],\n",
      "          [-2.2830e-02, -1.7756e-01,  1.9844e-01,  4.0105e-02, -1.1289e-01],\n",
      "          [ 1.3024e-01, -8.2999e-02,  8.4428e-02, -1.6838e-01, -3.6262e-02],\n",
      "          [-3.1718e-02,  1.9188e-01, -3.9674e-02, -1.0715e-01,  4.2148e-04]]],\n",
      "\n",
      "\n",
      "        [[[-1.4420e-03,  1.1310e-01, -3.5849e-02, -5.1461e-02, -5.5563e-03],\n",
      "          [-6.5541e-02,  1.9661e-02, -8.1799e-02,  2.5408e-02,  1.5485e-01],\n",
      "          [-1.7482e-01,  1.1213e-01,  1.3753e-01, -1.5496e-01,  1.3725e-02],\n",
      "          [ 1.2337e-01, -1.9706e-01,  2.3505e-02, -1.4323e-01,  1.8163e-01],\n",
      "          [ 1.8559e-01, -1.5738e-01,  1.3717e-01, -7.8642e-02, -1.1303e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.1396e-01,  7.7223e-02,  1.6135e-01,  3.7449e-03,  2.7801e-02],\n",
      "          [-1.5004e-01,  2.8224e-02, -1.4562e-01, -1.9518e-01,  3.1742e-02],\n",
      "          [ 5.2620e-02, -7.8785e-02,  1.7468e-01,  1.3901e-01, -1.9227e-01],\n",
      "          [-1.9531e-02,  1.2744e-01,  1.4613e-01,  9.0536e-04,  1.4656e-01],\n",
      "          [ 1.3569e-01,  6.8680e-02,  1.3985e-01, -1.5142e-02,  1.7601e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 5.6564e-02,  1.4796e-01,  1.0956e-01, -1.3249e-02, -1.6182e-02],\n",
      "          [-1.0642e-02,  1.0982e-01,  3.3045e-02, -1.0342e-02, -7.5365e-02],\n",
      "          [-8.1747e-02, -7.1110e-02,  1.4122e-01, -1.9685e-02,  1.3650e-01],\n",
      "          [ 1.8075e-02, -1.3517e-01, -1.6820e-01, -3.3523e-02,  1.0637e-01],\n",
      "          [ 1.6039e-01,  1.5209e-01,  8.3512e-02, -1.7399e-01,  1.4532e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1874e-01, -1.1207e-01,  1.3239e-01, -4.0610e-02,  1.2429e-02],\n",
      "          [ 1.2115e-01,  1.7422e-01,  1.6748e-02, -1.9381e-02,  5.4866e-02],\n",
      "          [ 1.2893e-01, -1.5956e-01, -9.7631e-02,  1.6200e-01,  3.4774e-02],\n",
      "          [-9.0139e-02, -5.8520e-02,  1.2816e-01,  1.4652e-01,  1.9550e-01],\n",
      "          [-1.5288e-01,  2.4104e-03,  1.8423e-01,  2.6435e-02, -7.0853e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.9066e-01,  1.9866e-01, -1.5194e-01,  9.5388e-02,  9.2195e-02],\n",
      "          [ 1.4634e-01, -1.3925e-01, -1.9722e-01,  2.9192e-02,  1.7233e-01],\n",
      "          [ 8.5037e-02,  1.5451e-01,  3.4850e-02, -1.8007e-01,  5.6773e-02],\n",
      "          [ 1.2711e-01,  1.8537e-01,  1.2328e-01, -3.2834e-02, -1.9241e-01],\n",
      "          [-1.8012e-01, -1.6196e-01,  1.5938e-01,  1.4263e-01, -8.1893e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6710e-01,  1.3547e-01, -1.3716e-01, -3.6652e-02,  1.3119e-01],\n",
      "          [ 3.2266e-02, -1.7501e-01, -7.5260e-02,  3.7379e-02,  1.0307e-01],\n",
      "          [-4.2182e-02, -3.5978e-02, -4.1455e-02,  1.9548e-01,  6.4439e-02],\n",
      "          [ 1.4917e-01, -1.1900e-02, -7.4314e-02, -1.9628e-01,  8.8212e-02],\n",
      "          [-3.1516e-03,  1.9581e-01,  1.5804e-01, -9.9766e-02,  8.6388e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 5.1080e-02, -1.7710e-01,  1.7666e-01,  1.4711e-02,  1.4279e-01],\n",
      "          [ 4.1502e-02,  1.5445e-01,  4.3531e-02,  1.3078e-01,  5.0903e-02],\n",
      "          [-1.1055e-01, -4.6960e-02,  6.8150e-02, -9.5687e-02, -1.2991e-01],\n",
      "          [-1.7807e-02, -9.5364e-03, -1.9830e-01, -1.6651e-01, -1.0530e-01],\n",
      "          [-1.1425e-01,  1.0091e-01,  1.8195e-01, -1.4276e-01, -1.2122e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.6800e-01, -1.0352e-01,  1.0601e-01,  1.8992e-01,  1.2521e-01],\n",
      "          [ 1.0493e-01, -1.2123e-01,  4.5985e-02, -1.1929e-01,  1.7097e-01],\n",
      "          [-8.4136e-02,  1.6370e-01,  9.6254e-02, -4.9536e-02, -5.2927e-02],\n",
      "          [-1.3102e-01,  7.3473e-02, -3.0236e-02, -1.9864e-01,  1.6833e-01],\n",
      "          [-1.6217e-01, -6.4154e-02, -1.3888e-01, -3.7458e-02,  1.1824e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5211e-01, -2.2679e-02,  1.3281e-01,  1.7465e-01, -1.6673e-01],\n",
      "          [-1.8950e-01, -1.6665e-01,  1.3832e-01, -1.2088e-01,  2.2868e-02],\n",
      "          [ 5.8324e-02,  6.1393e-02, -5.5257e-03, -1.5604e-01, -3.1596e-02],\n",
      "          [-6.9574e-02, -1.8253e-01,  4.7219e-02, -1.0891e-01,  1.2402e-01],\n",
      "          [-1.2912e-02, -3.5511e-02, -3.9404e-02, -1.1937e-01,  2.7835e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.2076e-01, -1.9195e-01, -1.2499e-01,  1.7159e-01, -1.6492e-01],\n",
      "          [-2.2347e-02, -1.3364e-01,  1.3218e-01, -5.1220e-02, -2.3797e-02],\n",
      "          [ 1.5902e-01, -6.7019e-02, -3.0269e-02,  8.9569e-02, -8.4086e-02],\n",
      "          [-1.2002e-01, -1.2911e-01,  4.6293e-02,  6.8403e-02, -1.0838e-01],\n",
      "          [ 9.8964e-02, -1.7218e-01, -8.7337e-02, -1.6771e-02,  1.5933e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.3245e-02, -6.6239e-02,  1.7755e-01,  1.9552e-01,  1.4676e-01],\n",
      "          [-1.9448e-01,  3.7593e-02, -1.4072e-01,  2.6547e-02, -6.7935e-02],\n",
      "          [-4.9412e-02,  1.1677e-02,  1.6416e-02,  1.0067e-01,  1.2366e-01],\n",
      "          [-1.9980e-02,  4.0555e-02,  4.6871e-02,  1.2406e-01, -5.3163e-03],\n",
      "          [ 7.2210e-02,  1.8822e-01,  4.2592e-02,  1.4741e-01, -8.6524e-02]]]])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=512, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all relevant packages\n",
    "import torch, torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from backpack import backpack, extend\n",
    "from backpack.extensions import DiagHessian\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.nn import functional as F\n",
    "\n",
    "s=9\n",
    "np.random.seed(s)\n",
    "torch.manual_seed(s)\n",
    "torch.cuda.manual_seed(s)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting zip archive\n",
      "Downloading http://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip to /Users/moreez/data/emnist/EMNIST/raw/emnist.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 561463296/561753746 [01:56<00:00, 5237881.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/moreez/data/emnist/EMNIST/raw/emnist.zip to /Users/moreez/data/emnist/EMNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "561758208it [02:10, 5237881.55it/s]                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing byclass\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/distiller/project/conda/conda-bld/pytorch_1587428061935/work/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing bymerge\n"
     ]
    }
   ],
   "source": [
    "# data wrangling: load Fashion-MNIST\n",
    "EMNIST_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "EMNIST_train = torchvision.datasets.EMNIST(\n",
    "        '~/data/emnist',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=EMNIST_transform,\n",
    "        split = 'mnist')\n",
    "\n",
    "\n",
    "\n",
    "EMNIST_test = torchvision.datasets.EMNIST(\n",
    "        '~/data/emnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=EMNIST_transform,\n",
    "        split = 'mnist')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at the data to verify\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    #plt.imshow(npimg)\n",
    "    plt.show()\n",
    "\n",
    "images = EMNIST_train.data[:10].view(10, 1, 28, 28)\n",
    "imshow(torchvision.utils.make_grid(images, nrow=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emnist_train_loader = torch.utils.data.dataloader.DataLoader(\n",
    "    EMNIST_train,\n",
    "    batch_size=128,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "emnist_test_loader = torch.utils.data.dataloader.DataLoader(\n",
    "    EMNIST_test,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the network\n",
    "def NN(num_classes=10):\n",
    "    \n",
    "    features = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(1, 32, 5),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.MaxPool2d(2,2),\n",
    "        torch.nn.Conv2d(32, 32, 5),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.MaxPool2d(2,2),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(4 * 4 * 32, num_classes)\n",
    "    )\n",
    "    return(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the training routine\n",
    "emnist_model = NN(num_classes=10)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "emnist_train_optimizer = torch.optim.Adam(emnist_model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "#dont use SGD, it is way worse than Adam here\n",
    "EMNIST_PATH = \"EMNIST_weights_seed={}.pth\".format(s)\n",
    "#print(FMNIST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get accuracy\n",
    "def get_accuracy(output, targets):\n",
    "    \"\"\"Helper function to print the accuracy\"\"\"\n",
    "    predictions = output.argmax(dim=1, keepdim=True).view_as(targets)\n",
    "    return predictions.eq(targets).float().mean().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the training routine and save the model at FMNIST_PATH\n",
    "\n",
    "def train(verbose=False, num_iter=5):\n",
    "    max_len = len(emnist_train_loader)\n",
    "    for iter in range(num_iter):\n",
    "        for batch_idx, (x, y) in enumerate(emnist_train_loader):\n",
    "            output = emnist_model(x)\n",
    "\n",
    "            accuracy = get_accuracy(output, y)\n",
    "\n",
    "            loss = loss_function(output, y)\n",
    "            loss.backward()\n",
    "            emnist_train_optimizer.step()\n",
    "            emnist_train_optimizer.zero_grad()\n",
    "\n",
    "            if verbose:\n",
    "                if batch_idx % 10 == 0:\n",
    "                    print(\n",
    "                        \"Iteration {}; {}/{} \\t\".format(iter, batch_idx, max_len) +\n",
    "                        \"Minibatch Loss %.3f  \" % (loss) +\n",
    "                        \"Accuracy %.0f\" % (accuracy * 100) + \"%\"\n",
    "                    )\n",
    "\n",
    "    print(\"saving model at: {}\".format(EMNIST_PATH))\n",
    "    torch.save(emnist_model.state_dict(), EMNIST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#after training it once, comment this out to save time if you rerun the entire script\n",
    "#train(verbose=True, num_iter=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict in distribution\n",
    "EMNIST_PATH = \"EMNIST_weights_seed={}.pth\".format(s)\n",
    "\n",
    "emnist_model = NN(num_classes=10)\n",
    "print(\"loading model from: {}\".format(EMNIST_PATH))\n",
    "emnist_model.load_state_dict(torch.load(EMNIST_PATH))\n",
    "emnist_model.eval()\n",
    "\n",
    "acc = []\n",
    "\n",
    "max_len = len(emnist_test_loader)\n",
    "for batch_idx, (x, y) in enumerate(emnist_test_loader):\n",
    "        output = emnist_model(x)\n",
    "        accuracy = get_accuracy(output, y)\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(\n",
    "                \"Batch {}/{} \\t\".format(batch_idx, max_len) + \n",
    "                \"Accuracy %.0f\" % (accuracy * 100) + \"%\"\n",
    "            )\n",
    "        acc.append(accuracy)\n",
    "    \n",
    "avg_acc = np.mean(acc)\n",
    "print('overall test accuracy on EMNIST: {:.02f} %'.format(avg_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace approximation of the weights\n",
    "* we use the BackPACK package to approximate the Hessian of the parameters. Especially look at the DiagHessian() method.\n",
    "* we do one iteration over the entire training set and use the mean of the Hessian of the mini-batches as the best approximation of the Hessian.\n",
    "* we add a prior variance to our Hessian. The precision is 1 over the variance. we use a prior precision of 10, 20, and 50 (or variance of 1/10, 1/20, 1/50).\n",
    "    * edit: I will use precicisions of 10, 60, 120, 1000 in the following\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Hessian_NN(model, train_loader, prec0, device='cpu', verbose=True):\n",
    "    lossfunc = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    extend(lossfunc, debug=False)\n",
    "    extend(model, debug=False)\n",
    "\n",
    "    Cov_diag = []\n",
    "    for param in model.parameters():\n",
    "        ps = param.size()\n",
    "        print(\"parameter size: \", ps)\n",
    "        Cov_diag.append(torch.zeros(ps, device=device))\n",
    "        #print(param.numel())\n",
    "\n",
    "    #var0 = 1/prec0\n",
    "    max_len = len(train_loader)\n",
    "\n",
    "    with backpack(DiagHessian()):\n",
    "\n",
    "        for batch_idx, (x, y) in enumerate(train_loader):\n",
    "\n",
    "            if device == 'cuda':\n",
    "                x, y = x.float().cuda(), y.long().cuda()\n",
    "\n",
    "            model.zero_grad()\n",
    "            lossfunc(model(x), y).backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Hessian of weight\n",
    "                for idx, param in enumerate(model.parameters()):\n",
    "\n",
    "                    H_ = param.diag_h\n",
    "                    #add prior here\n",
    "                    H_ += prec0 * torch.ones(H_.size())\n",
    "                    H_inv = torch.sqrt(1/H_) #<-- standard deviation\n",
    "                    #H_inv = 1/H_              #<-- variance \n",
    "\n",
    "                    rho = 1-1/(batch_idx+1)\n",
    "\n",
    "                    Cov_diag[idx] = rho*Cov_diag[idx] + (1-rho)* H_inv\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"Batch: {}/{}\".format(batch_idx, max_len))\n",
    "    \n",
    "    return(Cov_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EMNIST_NN_Std_prec_00001 = get_Hessian_NN(model=emnist_model, train_loader=emnist_train_loader, prec0=0.0001,verbose=False)\n",
    "#torch.save(EMNIST_NN_Std_prec_00001, 'Hessian_prec00001_EMNIST.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMNIST_NN_Std_prec_00001 = torch.load('Hessian_prec00001_KMNIST.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we want to look at the single layers of our network, and how they behave w.r.t. the variance\n",
    "* every tensor represents one of the six layers of out network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize the first layer of our networks in a heatmap\n",
    "* therefore we put the tensor in the right form/dimensions, by concatening all of its included arrays and then reshaping the tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "def visualize(tensor):\n",
    "    output = tensor[0][0]\n",
    "    for i in range(1, len(tensor[0])):\n",
    "        output = np.concatenate((output, tensor[0][i]))\n",
    "    output = output.transpose(2, 0, 1).reshape(5, -1)\n",
    "    heatmap = sns.heatmap(output)\n",
    "    plt.xticks = (np.arange(0, step=20))\n",
    "    plt.show()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize(MNIST_NN_Hessian_diag_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meancalc(Hessian_diag_x):\n",
    "    i = 0 \n",
    "    for name, parameter in emnist_model.named_parameters():\n",
    "        mean = torch.mean(Hessian_diag_x[i])\n",
    "        print(\"mean variance of layer {0:s}: {1:.4f}\".format(name, mean.item()))\n",
    "        i += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meancalc(Hessian_diag_x=EMNIST_NN_Std_prec_00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(EMNIST_NN_Std_prec_00001[4], cmap='gist_stern',extent=[0,512,0,1],  aspect='auto')\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(1, 512, 32));\n",
    "ax.set_xticklabels(np.arange(1, 32, 2));\n",
    "ax.set_title('seed {}'.format(s))\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#plt.savefig('linear_seed1000={}'.format(s))\n",
    "\n",
    "plt.hist(EMNIST_NN_Std_prec_00001[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mnist_number = 9\n",
    "linear_layer_index = 4 #linear layer has index 4\n",
    "linear_layer = MNIST_NN_Hessian_diag_120[linear_layer_index][mnist_number]\n",
    "\n",
    "#reshape the flattened array to 32* (4x4)\n",
    "layer_split = np.array_split(np.array(linear_layer), 32)\n",
    "for i in range(len(layer_split)-1):\n",
    "    layer_split[i] = np.reshape(layer_split[i], (4, -1))\n",
    "\n",
    "#plot setup\n",
    "fig, axs = plt.subplots(4,8, figsize=(20, 15))\n",
    "fig.subplots_adjust(hspace = .001, wspace=.001)\n",
    "axs = axs.ravel()\n",
    "\n",
    "#iterate through the features and plot them\n",
    "for i in range(len(layer_split)):\n",
    "    layer_split[i] = np.reshape(layer_split[i], (4, -1))\n",
    "    axs[i].imshow(layer_split[i])\n",
    "    axs[i].set_title('feature '+str(i+1))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, parameters in emnist_model.named_parameters():\n",
    "    if name == '7.weight':\n",
    "        a = parameters\n",
    "\n",
    "b = a.detach().numpy()\n",
    "\n",
    "plt.imshow(b, cmap='prism',extent=[0,512,0,1],  aspect='auto')\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(1, 512, 32));\n",
    "ax.set_xticklabels(np.arange(1, 32, 2));\n",
    "ax.set_title('weight {}'.format(s))\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "im = plt.show()\n",
    "plt.hist(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_hist = []\n",
    "target_feature = 9\n",
    "for i in range(10):\n",
    "    weight = a[i].detach().numpy()\n",
    "    weight = np.array_split(np.array(weight), 32)\n",
    "    to_hist.append(weight[target_feature -1])\n",
    "plt.hist(to_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "observe = [1, 8, 13, 14, 17, 24]\n",
    "f,c = plt.subplots(3,2)\n",
    "plt.figure(figsize=(20,20))\n",
    "c = c.ravel()\n",
    "for i in range(10):\n",
    "    test = np.array_split(a[i].detach().numpy(), 32)\n",
    "for idx, ax in enumerate(c):\n",
    "    ax.set_title(str(observe[idx] +1))\n",
    "    ax.hist(test[observe[idx]])\n",
    "plt.tight_layout()\n",
    "        \n",
    "    \n",
    "#for idx,ax in enumerate(a):\n",
    " #   ax.hist(test[observe[idx]])\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
